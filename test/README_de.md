# LangChain End-to-End Tests

Dieses Verzeichnis enthÃ¤lt vollstÃ¤ndige End-to-End-Testskripte der LangChain-Architektur zur ÃœberprÃ¼fung des gesamten Prozesses von der Frontend-Benutzereingabe bis zur LLM-Antwort.

## ğŸ“ Dateistruktur

```
test/
â”œâ”€â”€ end-to-end-langchain-test.js    # Haupt-Testskript
â”œâ”€â”€ run-e2e-test.js                 # Test-AusfÃ¼hrungsskript
â”œâ”€â”€ .env.example                    # Beispiel fÃ¼r Umgebungskonfiguration
â”œâ”€â”€ .env                           # TatsÃ¤chliche Umgebungskonfiguration (muss erstellt werden)
â”œâ”€â”€ test-results/                  # Testergebnis-Verzeichnis
â”‚   â”œâ”€â”€ langchain-e2e-test-results.json
â”‚   â””â”€â”€ langchain-e2e-test-report.html
â””â”€â”€ README.md                      # Dieses Dokument
```

## ğŸš€ Schnellstart

### 1. Umgebungskonfiguration

Vor der ersten AusfÃ¼hrung mÃ¼ssen Sie die Umgebungsvariablen konfigurieren:

```bash
# Beispiel fÃ¼r Umgebungskonfiguration kopieren
cp .env.example .env

# .env-Datei bearbeiten, um notwendige Konfigurationen vorzunehmen
# Besonders OPENAI_API_KEY (wenn Sie echte LLM-Aufrufe testen)
```

### 2. Tests AusfÃ¼hren

```bash
# VollstÃ¤ndige End-to-End-Tests ausfÃ¼hren
node run-e2e-test.js

# Nur Umgebungskonfiguration Ã¼berprÃ¼fen
node run-e2e-test.js --check

# Echte LLM-Aufrufe aktivieren (benÃ¶tigt gÃ¼ltigen API-SchlÃ¼ssel)
node run-e2e-test.js --real-llm

# Webserver-Port angeben
node run-e2e-test.js --port 8080

# Detaillierter Ausgabemodus
node run-e2e-test.js --verbose
```

### 3. Testbericht Anzeigen

Nach Abschluss der Tests wird automatisch ein Webserver gestartet, um den Testbericht anzuzeigen:

- Standard-Zugriffs-URL: http://localhost:3001
- API-Endpunkt: http://localhost:3001/api/test-results

## ğŸ“Š Testinhalt

### Testsprachen

Die Tests decken die folgenden 7 Sprachen ab:
- Chinesisch (zh-CN)
- Englisch (en-US) 
- Japanisch (ja)
- Koreanisch (ko)
- Spanisch (es-ES)
- Portugiesisch (pt-BR)
- FranzÃ¶sisch (fr)

### TestfÃ¤lle

Jede Sprache umfasst 5 TestfÃ¤lle:

1. **get-flow Tool-Trigger** - Test des SchlÃ¼sselworts "aktueller Fluss"
2. **get-node-info Tool-Trigger** - Test des SchlÃ¼sselworts "aktueller Knoten"
3. **get-settings Tool-Trigger** - Test des SchlÃ¼sselworts "aktuelle Konfiguration"
4. **get-diagnostics Tool-Trigger** - Test des SchlÃ¼sselworts "aktuelle Diagnose"
5. **NatÃ¼rlichsprachliche Unterhaltung** - Test "Node-RED vorstellen" (ohne Tool-Trigger)

### Aufgezeichnete Wichtige Informationen

Jeder Testfall zeichnet die folgenden Informationen auf:

- **a. Benutzereingabetext** - Simulierter Originaltext, den der Benutzer auf der Seite eingegeben hat
- **b. Erkanntes SchlÃ¼sselwort** - SchlÃ¼sselwort, das LangChain empfangen und identifiziert hat
- **c. Tool-Aufruf-Bestimmung** - Entscheidung des Systems, ein Tool aufzurufen
- **d. Tool-Typ und RÃ¼ckgabeinhalt** - Spezifisches aufgerufenes Tool und dessen RÃ¼ckgabeergebnis
- **e. Verketteter newHuman-Prompt an LLM gesendet** - Finaler Benutzer-Prompt an LLM gesendet
- **f. System-Prompt an LLM gesendet** - System-Level-Prompt
- **g. LLM-Antwort** - Antwortergebnis des groÃŸen Sprachmodells

## ğŸ”§ ErklÃ¤rung der Umgebungsvariablen

### Erforderliche Konfiguration

```bash
# OpenAI API-SchlÃ¼ssel (fÃ¼r echte LLM-Aufrufe)
OPENAI_API_KEY=your_openai_api_key_here

# Node-RED Umgebungssimulation
TEST_FLOW_ID=test-flow-123
TEST_NODE_ID=test-node-456
TEST_CONFIG_NODE_ID=test-config-node
```

### Optionale Konfiguration

```bash
# LLM-Anbieter-Konfiguration
TEST_LLM_PROVIDER=openai
TEST_LLM_MODEL=gpt-3.5-turbo

# Webserver-Port
TEST_WEB_PORT=3001

# Echte LLM-Aufrufe aktivieren
ENABLE_REAL_LLM_CALLS=false

# Debug-Konfiguration
DEBUG_MODE=true
LOG_LEVEL=info
```

## ğŸ“ˆ Testbericht

### Web-Bericht

Der nach Abschluss der Tests generierte HTML-Bericht umfasst:

- **Test-Zusammenfassung** - Allgemeine statistische Informationen
- **Sprachspezifische Tabellen** - Detaillierte Testergebnisse fÃ¼r jede Sprache
- **Status-Anzeige** - Erfolg/Fehler-Status
- **Responsive Design** - Anpassung an verschiedene BildschirmgrÃ¶ÃŸen

### JSON-Daten

Die rohen Testdaten werden im JSON-Format gespeichert und kÃ¶nnen verwendet werden fÃ¼r:

- Automatisierte Analyse
- Integration in CI/CD-Pipelines
- Generierung benutzerdefinierter Berichte

## ğŸ› ï¸ Technische Architektur

### Testprozess

1. **Umgebungsinitialisierung** - ÃœberprÃ¼fung von Konfiguration, AbhÃ¤ngigkeiten und Umgebungsvariablen
2. **Frontend-Simulation** - Simulation von Benutzereingabe und SchlÃ¼sselworterkennung
3. **Backend-Verarbeitung** - Aufruf des LangChain Managers zur Anfrageverarbeitung
4. **Tool-AusfÃ¼hrung** - Simulation oder tatsÃ¤chliche AusfÃ¼hrung verwandter Tools
5. **LLM-Interaktion** - Prompt-Konstruktion und LLM-Antwort-Erhaltung
6. **Ergebnis-Aufzeichnung** - Speicherung vollstÃ¤ndiger Verarbeitungsketten-Informationen
7. **Bericht-Generierung** - Generierung von Web-Berichten und JSON-Daten

### Simulationskomponenten

- **Mock Node-RED** - Simulation der Node-RED-AusfÃ¼hrungsumgebung
- **Mock Tools** - Simulation von Tool-AusfÃ¼hrungsergebnissen
- **Mock LLM** - Optionale Simulation von LLM-Antworten

## ğŸ” Fehlerbehebung

### HÃ¤ufige Probleme

1. **Umgebungsvariablen nicht konfiguriert**
   ```bash
   # ÃœberprÃ¼fen, ob .env-Datei existiert und korrekt konfiguriert ist
   node run-e2e-test.js --check
   ```

2. **Fehlende AbhÃ¤ngigkeiten**
   ```bash
   # Notwendige AbhÃ¤ngigkeiten installieren
   npm install express dotenv
   ```

3. **UngÃ¼ltiger API-SchlÃ¼ssel**
   ```bash
   # Im Simulationsmodus testen
   node run-e2e-test.js
   # Oder ENABLE_REAL_LLM_CALLS=false konfigurieren
   ```

4. **Port in Verwendung**
   ```bash
   # Anderen Port angeben
   node run-e2e-test.js --port 8080
   ```

### Debug-Modus

```bash
# Detaillierte Ausgabe aktivieren
node run-e2e-test.js --verbose

# Oder in .env konfigurieren
DEBUG_MODE=true
LOG_LEVEL=debug
```

## ğŸ“ Erweiterungsentwicklung

### Neue Sprache HinzufÃ¼gen

1. Sprachcode zu `TEST_CONFIG.languages` hinzufÃ¼gen
2. Entsprechende TestfÃ¤lle zu `TEST_CONFIG.testCases` hinzufÃ¼gen
3. ÃœberprÃ¼fen, dass entsprechende Sprachkonfigurationsdatei existiert

### Neuen Testfall HinzufÃ¼gen

```javascript
// Zu den TestfÃ¤llen der entsprechenden Sprache hinzufÃ¼gen
{ 
    keyword: 'neues SchlÃ¼sselwort', 
    expectedTool: 'new-tool', 
    description: 'Beschreibung des neuen Testfalls' 
}
```

### Benutzerdefinierte Tool-Simulation

Simulationsergebnisse neuer Tools zum `mockToolResults`-Objekt in der `executeTestCase`-Funktion hinzufÃ¼gen.

## ğŸ“„ Lizenz

Dieses Testskript folgt derselben Lizenz wie das Hauptprojekt.

## ğŸ¤ Beitrag

Wir begrÃ¼ÃŸen Issues und Pull Requests zur Verbesserung des Testskripts!

---

**Hinweis**: Dieses Testskript basiert auf dem Architekturdesign, das im Dokument `LANGCHAIN_ARCHITECTURE.md` beschrieben ist, und gewÃ¤hrleistet die Testabdeckung des vollstÃ¤ndigen Benutzerinteraktionsprozesses.